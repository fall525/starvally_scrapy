import time
from redis_manager import RedisManager
from db_manager import DBManager

# è§£å†³ä¸€ä¸‹å¿ƒåŠ¨ç¼ºå¤±çš„é—®é¢˜
class MonitorManager:
    def __init__(self, host="127.0.0.1", port=6379, db=0):
        self.redis_client = RedisManager(host, port, db).redis_client
    
    ### 1. è·å–çˆ¬è™«è¿›ç¨‹çŠ¶æ€ ###
    def get_active_pids(self):
        """è·å–æ‰€æœ‰æ´»è·ƒçš„çˆ¬è™«è¿›ç¨‹ID"""
        return self.redis_client.smembers("active_crawlers")

    def get_crawler_status(self, pid):
        """è·å–æŒ‡å®šçˆ¬è™«è¿›ç¨‹çš„çŠ¶æ€ä¿¡æ¯"""
        return self.redis_client.hgetall(f"crawler:status:{pid}")

    ### 2. è·å–ä»»åŠ¡é˜Ÿåˆ—çŠ¶æ€ ###
    def get_queue_size(self):
        """è·å–å¾…çˆ¬å–çš„URLæ•°é‡"""
        return self.redis_client.llen("url_queue")

    def get_failed_queue_size(self):
        """è·å–å¤±è´¥çš„URLæ•°é‡"""
        return self.redis_client.llen("failed_urls")
    
    def get_crawler_heartbeat(self, pid):
        """è·å–æŒ‡å®šçˆ¬è™«è¿›ç¨‹çš„æœ€åå¿ƒè·³æ—¶é—´"""
        return self.redis_client.get(f"crawler:heartbeat:{pid}")

    ### 3. ä»»åŠ¡çŠ¶æ€ç»Ÿè®¡ ###
    def get_all_status_count(self):
        """ç»Ÿè®¡ä¸åŒçˆ¬å–çŠ¶æ€çš„URLæ•°é‡ (crawling, done, failed)"""
        status_count = {
            "crawling": 0,
            "done": 0,
            "failed": 0
        }
        all_status = self.redis_client.hgetall("url_status")
        for status in all_status.values():
            if status in status_count:
                status_count[status] += 1
        return status_count

monitor_manager = MonitorManager()
db_manager = DBManager()
HEARTBEAT_TIMEOUT = 20  # å¿ƒè·³è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

def show_queue_status():
    """æ˜¾ç¤ºä»»åŠ¡é˜Ÿåˆ—çŠ¶æ€"""
    url_count = monitor_manager.get_queue_size()
    failed_count = monitor_manager.get_failed_queue_size()
    print(f"ğŸ“¥ å¾…çˆ¬å– URL æ•°é‡: {url_count}, å¤±è´¥ URL æ•°é‡: {failed_count}")

def show_db_status():
    """æ˜¾ç¤º MongoDB æ•°æ®åº“çŠ¶æ€"""
    page_count = db_manager.count_pages()
    print(f"ğŸ“š å·²å­˜å‚¨é¡µé¢æ•°é‡: {page_count}")

def show_url_status_count():
    """æ˜¾ç¤ºä¸åŒçŠ¶æ€çš„URLæ•°é‡"""
    status_count = monitor_manager.get_all_status_count()
    print(f"ğŸ“Š URL çŠ¶æ€ç»Ÿè®¡: çˆ¬å–ä¸­: {status_count['crawling']} | å·²å®Œæˆ: {status_count['done']} | å¤±è´¥: {status_count['failed']}")

def show_crawler_status():
    """æ˜¾ç¤ºæ‰€æœ‰çˆ¬è™«è¿›ç¨‹çš„çŠ¶æ€ä¿¡æ¯"""
    active_pids = monitor_manager.get_active_pids()
    print(f"\nğŸ•µï¸â€â™‚ï¸ å½“å‰æ´»è·ƒçˆ¬è™«è¿›ç¨‹: {len(active_pids)} ä¸ª")
    
    for pid in active_pids:
        status_info = monitor_manager.get_crawler_status(pid)
        last_heartbeat = monitor_manager.get_crawler_heartbeat(pid)
        if status_info:
            status = status_info.get('status')
            current_url = status_info.get('current_url')
            last_active_time = status_info.get('last_active_time')
            
            # æ£€æµ‹å¿ƒè·³è¶…æ—¶
            if last_heartbeat:
                last_heartbeat = float(last_heartbeat)
                time_since_last_heartbeat = time.time() - last_heartbeat
                if time_since_last_heartbeat > HEARTBEAT_TIMEOUT:
                    status = "âš ï¸ æ— å“åº”"
            else:
                status = "âŒ å¿ƒè·³ä¸¢å¤±"

            print(f"ğŸ è¿›ç¨‹ {pid} | çŠ¶æ€: {status} | å½“å‰URL: {current_url} | ä¸Šæ¬¡æ´»åŠ¨: {last_active_time}")
        else:
            print(f"â“ æœªæ‰¾åˆ°è¿›ç¨‹ {pid} çš„çŠ¶æ€ä¿¡æ¯")

def monitor_status(interval=5):
    """ä¸»ç›‘æ§å¾ªç¯ï¼Œæ¯éš” interval ç§’åˆ·æ–°ä¸€æ¬¡çŠ¶æ€ä¿¡æ¯"""
    try:
        while True:
            print("\nğŸ“Š æ­£åœ¨åˆ·æ–°çˆ¬è™«çŠ¶æ€...")
            show_queue_status()
            show_db_status()
            show_url_status_count()
            show_crawler_status()
            time.sleep(interval)
    except KeyboardInterrupt:
        print("ğŸ›‘ ç›‘æ§ç¨‹åºå·²é€€å‡º")

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨çˆ¬è™«ç›‘æ§ç¨‹åº...")
    monitor_status()