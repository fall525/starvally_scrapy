import requests
from bs4 import BeautifulSoup
import time
from redis_manager import RedisManager
from db_manager import DBManager

# åˆå§‹åŒ– Redis å’Œ MongoDB è¿æ¥
redis_manager = RedisManager()
db_manager = DBManager()

# è®¾ç½® User-Agentï¼Œé˜²æ­¢å°ç¦
HEADERS = {"User-Agent": "Mozilla/5.0"}

# çˆ¬å–ç½‘é¡µæ–‡æœ¬å†…å®¹
# def fetch_text(url):
#     """è¯·æ±‚ç½‘é¡µå¹¶æå–çº¯æ–‡æœ¬å†…å®¹"""
#     try:
#         response = requests.get(url, headers=HEADERS, timeout=10)
#         if response.status_code == 200:
#             soup = BeautifulSoup(response.text, "html.parser")

#             # è·å–ä¸»è¦å†…å®¹åŒºåŸŸï¼ˆå»æ‰å¯¼èˆªã€è„šæ³¨ã€å¹¿å‘Šï¼‰
#             content_div = soup.find("div", {"id": "mw-content-text"})
#             if content_div:
#                 text = content_div.get_text(separator="\n", strip=True)
#             else:
#                 text = soup.get_text(separator="\n", strip=True)

#             return text, soup
#         else:
#             print(f"âŒ å¤±è´¥ {url} çŠ¶æ€ç : {response.status_code}")
#             return None, None
#     except Exception as e:
#         print(f"âš ï¸ çˆ¬å– {url} å¤±è´¥ï¼Œé”™è¯¯: {e}")
#         return None, None
def fetch_text(url):
    """è¯·æ±‚ç½‘é¡µå¹¶æå–çº¯æ–‡æœ¬å†…å®¹ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")

            # è·å–æ­£æ–‡åŒºåŸŸ
            content_div = soup.find("div", {"id": "mw-content-text"})
            if not content_div:
                return None, None
            
            # è§£ææ‰€æœ‰ <p> æ®µè½ï¼Œå¹¶æ‹¼æ¥æˆæ–‡æœ¬
            paragraphs = content_div.find_all(["p", "li"])
            cleaned_text = "\n\n".join(p.get_text(strip=True) for p in paragraphs)

            return cleaned_text, soup
        else:
            print(f"âŒ å¤±è´¥ {url} çŠ¶æ€ç : {response.status_code}")
            return None, None
    except Exception as e:
        print(f"âš ï¸ çˆ¬å– {url} å¤±è´¥ï¼Œé”™è¯¯: {e}")
        return None, None


# è§£æç½‘é¡µï¼Œæå–ç«™å†…é“¾æ¥
def parse_internal_links(soup, base_url):
    """è§£æ HTMLï¼Œæå–ç«™å†…é“¾æ¥"""
    links = set()

    for a_tag in soup.find_all("a", href=True):
        link = a_tag["href"]

        # åªä¿ç•™ç«™å†…é“¾æ¥ï¼ˆä»¥ / å¼€å¤´æˆ–åŒ…å« base_urlï¼‰
        if link.startswith("/"):
            link = base_url + link
        elif not link.startswith(base_url):
            continue  # è·³è¿‡å¤–éƒ¨é“¾æ¥

        links.add(link)

    return links

# å¤„ç† URL ä»»åŠ¡
def process_url():
    """ä¸»çˆ¬å–é€»è¾‘ï¼Œä» Redis é˜Ÿåˆ—è·å– URLï¼Œçˆ¬å–å¹¶å­˜å‚¨"""
    while True:
        url = redis_manager.pop_url()  # è·å–å¾…çˆ¬å– URL
        if url is None:
            print("ğŸŒŸ URL é˜Ÿåˆ—ä¸ºç©ºï¼Œç­‰å¾…æ–°ä»»åŠ¡...")
            time.sleep(5)
            continue

        if redis_manager.is_visited(url):
            print(f"ğŸ”„ {url} å·²çˆ¬å–ï¼Œè·³è¿‡")
            continue

        print(f"ğŸŒ æ­£åœ¨çˆ¬å–: {url}")
        redis_manager.set_status(url, "crawling")  # è®¾ç½®çŠ¶æ€

        text, soup = fetch_text(url)  # ä¸‹è½½é¡µé¢å¹¶æå–æ–‡æœ¬
        if text:
            links = parse_internal_links(soup, "https://zh.stardewvalleywiki.com")  # æå–ç«™å†…é“¾æ¥
            db_manager.save_page(url, text)  # å­˜å…¥ MongoDB
            # print(text[:10])

            redis_manager.mark_visited(url)  # æ ‡è®°å·²çˆ¬å–
            redis_manager.set_status(url, "done")  # æ›´æ–°çŠ¶æ€

            # æ·»åŠ æ–°é“¾æ¥åˆ°ä»»åŠ¡é˜Ÿåˆ—
            for link in links:
                if not redis_manager.is_visited(link):
                    redis_manager.push_url(link)

            print(f"âœ… çˆ¬å–æˆåŠŸ: {url} (æå– {len(links)} ä¸ªç«™å†…é“¾æ¥)")
        else:
            print(f"âŒ çˆ¬å–å¤±è´¥: {url}, åŠ å…¥å¤±è´¥é˜Ÿåˆ—")
            redis_manager.push_failed_url(url)
            redis_manager.set_status(url, "failed")

        time.sleep(1)  # æ§åˆ¶çˆ¬å–é—´éš”ï¼Œé¿å…è¿‡å¿«

# if __name__ == "__main__":
#     process_url()