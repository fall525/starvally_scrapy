import os
import time
import re
import requests
from bs4 import BeautifulSoup
import settings
from distributed_distribution_manager import DistributedDistributionManager
from db_manager import DBManager

# è·å–å½“å‰çˆ¬è™«è¿›ç¨‹çš„ ID å’ŒèŠ‚ç‚¹ ID (ç”¨äºè½®è¯¢åˆ†é…ç­–ç•¥)
pid = os.getpid()

# è®¾ç½® User-Agentï¼Œé˜²æ­¢å°ç¦
HEADERS = {"User-Agent": "Mozilla/5.0"}

# åˆå§‹åŒ–æ•°æ®åº“å’Œåˆ†å¸ƒå¼ç®¡ç†å™¨
db_manager = DBManager(
    host=settings.MONGODB_HOST,
    port=settings.MONGODB_PORT,
    db_name=settings.MONGODB_DB_NAME,
    col_name=settings.MONGODB_COLL_NAME
)

distribution_manager = DistributedDistributionManager(
    host=settings.REDIS_HOST,
    port=settings.REDIS_PORT,
    db=settings.REDIS_DB
)

# æ¨é€ç§å­ URL åˆ° Redis
def push_seed_urls():
    if distribution_manager.queue_size() == 0:
        for url in settings.SEED_URLS:
            if not distribution_manager.is_visited(url):
                distribution_manager.push_url(url)
                print(f"ğŸŒ± æ¨é€ç§å­ URL: {url}")
            else:
                print(f"ğŸš« URL å·²å­˜åœ¨: {url}")
    else:
        print("ğŸ“¥ Redis é˜Ÿåˆ—å·²æœ‰ä»»åŠ¡ï¼Œæ— éœ€æ¨é€ç§å­ URL")

# æ›´æ–°çˆ¬è™«çŠ¶æ€
def update_crawler_status(status, current_url=None):
    status_info = {
        "current_url": current_url if current_url else "idle",
        "status": status,
        "last_active_time": time.strftime("%Y-%m-%d %H:%M:%S")
    }
    distribution_manager.set_crawler_status(pid, status_info)
    distribution_manager.add_active_crawler(pid)

# å‘é€å¿ƒè·³ä¿¡å·
def send_heartbeat():
    distribution_manager.send_heartbeat(pid)

# è·å–ç½‘é¡µæ–‡æœ¬
def fetch_text(url):
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            content_div = soup.find("div", {"id": "mw-content-text"})
            if not content_div:
                return None, None

            paragraphs = content_div.find_all(["p", "li"])
            cleaned_text = "\n".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))

            return cleaned_text, soup
        else:
            print(f"âŒ å¤±è´¥ {url} çŠ¶æ€ç : {response.status_code}")
            return None, None
    except Exception as e:
        print(f"âš ï¸ çˆ¬å– {url} å¤±è´¥ï¼Œé”™è¯¯: {e}")
        return None, None

# è§£æå†…éƒ¨é“¾æ¥
def parse_internal_links(soup, base_url):
    links = set()
    for a_tag in soup.find_all("a", href=True):
        link = a_tag["href"]
        if link.startswith("/") and not link.startswith("//"):
            link = base_url + link
        elif not link.startswith(base_url):
            continue
        links.add(link)
    return links

# å¤„ç† URL é€»è¾‘
def process_url():
    push_seed_urls()  # å…ˆæ¨é€ç§å­ URL
    try:
        while True:
            send_heartbeat()

            while distribution_manager.distribute_url():
                pass  # å¤„ç†æ‰€æœ‰æœªåˆ†å‘çš„ URL

            url = distribution_manager.pop_url()
            if not url:
                print("ğŸŒŸ URL é˜Ÿåˆ—ä¸ºç©ºï¼Œç­‰å¾…æ–°ä»»åŠ¡...")
                update_crawler_status("idle")
                time.sleep(5)
                continue

            if distribution_manager.is_visited(url):
                print(f"ğŸ”„ {url} å·²çˆ¬å–ï¼Œè·³è¿‡")
                continue

            update_crawler_status("crawling", url)
            print(f"ğŸŒ æ­£åœ¨çˆ¬å–: {url}")
            distribution_manager.set_status(url, "crawling")

            text, soup = fetch_text(url)
            if text:
                links = parse_internal_links(soup, "https://stardewvalleywiki.com")
                db_manager.save_page(url, text)

                distribution_manager.mark_visited(url)
                distribution_manager.set_status(url, "done")
                update_crawler_status("done", url)

                for link in links:
                    if not distribution_manager.is_visited(link):
                        distribution_manager.push_url_to_master(link)

                print(f"âœ… çˆ¬å–æˆåŠŸ: {url} (æå– {len(links)} ä¸ªç«™å†…é“¾æ¥)")
            else:
                print(f"âŒ çˆ¬å–å¤±è´¥: {url}, åŠ å…¥å¤±è´¥é˜Ÿåˆ—")
                distribution_manager.push_failed_url(url)
                distribution_manager.set_status(url, "failed")
                update_crawler_status("failed", url)

            time.sleep(1)
    finally:
        distribution_manager.clear_crawler_status(pid)
        distribution_manager.remove_active_crawler(pid)

# å¯åŠ¨çˆ¬è™«
def start_crawler():
    update_crawler_status("idle")  # å¯åŠ¨æ—¶æ³¨å†Œè¿›ç¨‹çŠ¶æ€
    process_url()

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨åˆ†å¸ƒå¼çˆ¬è™«ç³»ç»Ÿ")
    start_crawler()
